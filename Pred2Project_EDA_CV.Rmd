---
title: "Pred2Project_EDA"
author: "Rachel Rosenberg"
date: "2/13/2019"
output: word_document
---

```{r setup, include=FALSE}

library(tidyverse)
library(readxl)
library(plyr)
library(dplyr)
```



## Import Data
```{r}
#Import the Food Inspection-related data

Garbage_Carts <- read_csv("Aggregated Data/311_Service_Requests_-_Garbage_Carts.csv")
Sanitation <- read_csv("Aggregated Data/311_Service_Requests_-_Sanitation_Code_Complaints.csv")
Business_Licenses <- read_csv("Aggregated Data/Business_Licenses.csv")
Food_Inspections <- read_csv("Aggregated Data/Food_Inspections.csv")
Affordable_Housing <- read_csv("Aggregated Data/Affordable_Rental_Housing_Developments.csv")
Census=read.csv('Aggregated Data/Census_Data_-_Selected_socioeconomic_indicators_in_Chicago__2008___2012.csv')
Weather=read.csv('Aggregated Data/Weather.csv')
```

```{r}
library(lubridate)

#Create variable for month of inspection
Food_Inspections$inspection_month <- month(as.POSIXlt(Food_Inspections$`Inspection Date`, format="%m/%d/%Y"))

#Adding weather month
Weather$weather_month<-month(as.POSIXlt(Weather$date, format="%Y-%m-%d"))
Weather=Weather[c(-1,-6)]
Weather <- aggregate(.~weather_month, Weather,mean)

#Filter Food Inspection Data for usable columns
Food_Inspections_filtered <- Food_Inspections[,c(1:2,4,5,6,10:13,18)]

#Filter food inspection results for pass, fail, or pass w/ conditions
Food_Inspections_filtered <- Food_Inspections_filtered[Food_Inspections_filtered$Results == "Pass" | Food_Inspections_filtered$Results == "Fail" | Food_Inspections_filtered$Results == "Pass w/ Conditions" ,]

#Create Binary Classifyer for Pass/Fail (will need to adjust later)
Food_Inspections_filtered$Results_Numeric <- ifelse(Food_Inspections_filtered$Results =="Fail", 0, 1)
  
#filter for usable business license info
License_Desc <- Business_Licenses[,c(2,11,16)]

#filter for usable sanitation info then aggregate for each ward
Sanitation_filtered <- Sanitation[,c(4,11)]
Sanitation_filtered <- aggregate(`Service Request Number` ~ Ward, Sanitation_filtered, function(x) length(unique(x)))
colnames(Sanitation_filtered)[2] <- "Num_Sanitation_Requests"

#filter for usable garbage info then aggregate for each ward
Garbage_filtered <- Garbage_Carts[,c(4,13)]
Garbage_filtered <- aggregate(`Service Request Number` ~ Ward, Garbage_filtered, function(x) length(unique(x)))
colnames(Garbage_filtered)[2] <- "Num_Garbage_Requests"

Affordable_Housing_by_ZIP <- aggregate(Affordable_Housing$Units, by=list(Category=Affordable_Housing$`Zip Code`), FUN=sum)
colnames(Affordable_Housing_by_ZIP) <- c("Zip", "Affordable_Housing_Units")

#Joining Per Capita by Community Area
CensusData<-Census[,c(1,8)]
colnames(CensusData)[1]<-'Community Area Number'
Affordable_Housing_perCapInc<-merge(CensusData,Affordable_Housing, by='Community Area Number')
Zip_perCapInc<-Affordable_Housing_perCapInc[,c(2,7)]
colnames(Zip_perCapInc)[2]<-'Zip'
Zip_perCapInc <- aggregate(PER.CAPITA.INCOME ~ Zip, Zip_perCapInc, function(x) mean(x))
```

#join data together
```{r}

ModelData <- merge(Food_Inspections_filtered, License_Desc, by.x = "License #", by.y = "LICENSE ID")
ModelData <- merge(ModelData, Sanitation_filtered, by.x = "WARD", by.y = "Ward")
ModelData <- merge(ModelData, Garbage_filtered , by.x = "WARD", by.y = "Ward")
ModelData <- merge(ModelData, Affordable_Housing_by_ZIP , by.x = "Zip", by.y = "Zip")
ModelData$ReInspection<-ifelse(grepl('Re-Inspection',ModelData$`Inspection Type`),1,0)
ModelData<- merge(ModelData, Zip_perCapInc, by.x='Zip', by.y='Zip' )
ModelData<-merge(ModelData, Weather, by.x='inspection_month', by.y='weather_month')
```





#Create linear model
```{r}
ModelData1=ModelData[,c(12,14,16:18,20)]
ModelData1[,c(1,4)]=lapply(ModelData1[,c(1,4)],factor)

logit_model <- glm(Results_Numeric ~ ., family=binomial(link='logit'), data=ModelData1)
summary(logit_model)


```





```{r}
#Random forest model
ModelData1[,c(2,3,5,6)]<-sapply(ModelData1[,c(2,3,5,6)], function(x) (x-mean(x))/sd(x)) #standardize predictors


library(randomForest)
rForest1 <- randomForest(Results_Numeric~ ., data=ModelData1, mtry=3, ntree = 500, nodesize = 3, importance = TRUE)
plot(rForest1)  #plots OOB mse vs # trees
rForest1 #check the OOB mse and r^2
importance(rForest1); varImpPlot(rForest1)
par(mfrow=c(2,4))
for (i in c(2,3,5,6)) partialPlot(rForest1, pred.data=ModelData1, x.var = names(ModelData1)[i], xlab = names(ModelData1)[i], main=NULL) #creates "partial dependence" plots 
par(mfrow=c(1,1))
c(rForest1$mse[rForest1$ntree], sum((rForest1$predicted - ModelData1$Results_Numeric)^2)/nrow(ModelData1)) #both give the OOB MSE


```

```{r}
#######A function to determine the indices in a CV partition##################
CVInd <- function(n,K) {  #n is sample size; K is number of parts; returns K-length list of indices for each part
   m<-floor(n/K)  #approximate size of each part
   r<-n-m*K  
   I<-sample(n,n)  #random reordering of the indices
   Ind<-list()  #will be list of indices for all K parts
   length(Ind)<-K
   for (k in 1:K) {
      if (k <= r) kpart <- ((m+1)*(k-1)+1):((m+1)*k)  
         else kpart<-((m+1)*r+m*(k-r-1)+1):((m+1)*r+m*(k-r))
      Ind[[k]] <- I[kpart]  #indices for kth part of data
   }
   Ind
}

```


```{r}
#CV for GAM
library(mgcv)  #stands for “Mixed GAM Computation Vehicle”
ModelData2=ModelData1
ModelData2$ReInspection=as.numeric(ModelData2$ReInspection)
Nrep=2 #number of replicates of CV
K=3  #K-fold CV on each replicate
n.models =1 #number of different models to fit
n=nrow(ModelData2)
y=ModelData2$Results_Numeric
phat=matrix(0,n,n.models)
yhat=matrix(0,n,n.models) 
MC=matrix(0,Nrep,n.models)
for (j in 1:Nrep) {
  Ind=CVInd(n,K)
  for (k in 1:K) {
    out<-gam(Results_Numeric ~ s(Num_Sanitation_Requests) + s(Affordable_Housing_Units) + s(PER.CAPITA.INCOME) + s(temperatureMax), data=ModelData2[-Ind[[k]],], family=binomial())
             
    yhat[Ind[[k]],]<-ifelse((predict(out,ModelData2[Ind[[k]],]))>0,1,0)
  } #end of k loop
  MC[j,]=apply(yhat,2,function(x) sum(y!=x))/n
} #end of j loop
MCAve<- apply(MC,2,mean); MCAve #averaged mean square CV error
```

```{r}
out<-gam(Results_Numeric~s(Num_Sanitation_Requests)+s(Affordable_Housing_Units)+s(PER.CAPITA.INCOME)+s(temperatureMax), data=ModelData1, family=binomial(), sp=c(-1,-1,-1,-1,-1)) 
summary(out)
out$sp  ##estimated smoothing parameters for each constituent function 
yhat<-predict(out)
plot(yhat,ModelData1$Results_Numeric)  #probably quite a bit of overitting
##
par(mfrow=c(2,4))
plot(out)  #plot component functions
```

```{r}
#CV for KNN
Nrep=2 #number of replicates of CV
K=3  #K-fold CV on each replicate
n.models = 9 #number of different models to fit
n=nrow(ModelData1)
y=ModelData1$Results_Numeric
phat=matrix(0,n,n.models)
yhat=matrix(0,n,n.models) 
MC=matrix(0,Nrep,n.models)
for (j in 1:Nrep) {
  Ind=CVInd(n,K)
  i=1
  for (k in 1:K) {
    train=as.matrix(ModelData1[-Ind[[k]],2:6]) 
    test=as.matrix(ModelData1[Ind[[k]],2:6]) 
    ytrain=ModelData1[-Ind[[k]],1]
  for (kn in c(2,5,8)) {
    for (thres in c(0.25,0.5,0.75)){
      out=ann(train,test,kn,verbose=F) 
      ind=as.matrix(out$knnIndexDist[,1:kn]) 
      phat[Ind[[k]],i] = mean(apply(ind,1,function(x) sum(ytrain[x]==1)/length(ytrain[x])))
      yhat[Ind[[k]],i] = ifelse(phat[Ind[[k]],i]>thres, 1, 0)
      i=i+1
  }
  }
  } #end of k loop
  MC[j,]=apply(yhat,2,function(x) sum(y!=x))/n
} #end of j loop
MCAve<- apply(MC,2,mean); MCAve #averaged mean square CV error
```


```{r}
#K-Nearest Neighbors
library(yaImpute)
library(caret)

train<-as.matrix(ModelData1[,2:6])
test<-as.matrix(ModelData1[,2:6])
ytrain<-ModelData1[,1]
ytest<-ModelData1[,1]
K=10
out<-ann(train,test,K)
ind<-as.matrix(out$knnIndexDist[,1:K])
phat<-apply(ind,1,function(x) sum(ytrain[x]==1)/length(ytrain[x]))
yhat<- ifelse(phat>0.5, 1, 0)
plot(yhat,jitter(as.numeric(ytest==1),amount=.05))
table(yhat,ytest)

```

```{r}
#CV for NeuralNet
library(nnet)
Nrep<-3 #number of replicates of CV
K<-3  #K-fold CV on each replicate
n.models = 30 #number of different models to fit
n=nrow(ModelData1)
y<-ModelData1$Results_Numeric
yhat=matrix(0,n,n.models)
MSC<-matrix(0,Nrep,n.models) #miss classification
for (j in 1:Nrep) {
  Ind<-CVInd(n,K)
  for (k in 1:K) {
    for(l in 1:3){
      for(s in 1:10){
     out<-nnet(Results_Numeric~.,ModelData1[-Ind[[k]],],linout=F,skip=F, size=s, decay=10^(-l),maxit=100,trace=F)
     yhat[Ind[[k]],(l-1)*10+s]<-predict(out,ModelData1[Ind[[k]],],type="class")
    }
    }
  } #end of k loop
  MSC[j,]=apply(yhat,2,function(x) sum((y!=x)))/n
} #end of j loop
MSC
MSCAve<- apply(MSC,2,mean); MSCAve #averaged mean square CV error
```


```{r}
nfit2 <- nnet(logcost~.,data2,linout=T,skip=F, size=2, decay=0.01,maxit=1000,trace=F)
```


```{r}
#CV for SVM
library(e1071)
Nrep<-2 #number of replicates of CV
K<-3  #K-fold CV on each replicate
n.models = 3 #number of different models to fit
n=nrow(ModelData1)
y<-ModelData1$Results_Numeric
yhat=matrix(0,n,n.models)
MSC<-matrix(0,Nrep,n.models) #miss classification
for (j in 1:Nrep) {
  Ind<-CVInd(n,K)
  i=1
  for (k in 1:K) {
      for(s in c(1,5,10)){
        out<-svm(Results_Numeric ~ ., data = ModelData1, cost = s, scale = FALSE)
        yhat[Ind[[k]],i]<-predict(out,ModelData1[Ind[[k]],i],type="class")
        i=i+1
    }
  } #end of k loop
  MSC[j,]=apply(yhat,2,function(x) sum((y!=x)))/n
} #end of j loop
MSC
MSCAve<- apply(MSC,2,mean); MSCAve #averaged mean square CV error
```


```{r}
#SVM Model

out<-svm(Results_Numeric ~ ., data = ModelData1, cost = 5, scale = FALSE)
summary(out)
```











#Everything below here is old



## Join Health data and neighborhoods
```{r}
colnames(ZIPtoCA) <- c("ChicagoCA", "ZIPCode", "2010Pop")
health <- merge(health, ZIPtoCA, by.x = "Community.Area", by.y = "ChicagoCA")
health <- merge(health, ZIP, by.x = "ZIPCode", by.y = "Zip Code")
health <- health[ , c(1:4, 18, 5:17, 18:32)] # reorder columns

write.csv(health, file = "~/420 - Predictive II/Aggregated Data/healthWithZIPSandCAs.csv")
```

## Create full matrix of predictors
```{r}
allPreds <- health
names(allPreds)
head(allPreds)
head(grocery)

# Merge in park_areas
allPreds <- merge(allPreds, park_areas, by.x = "ZIPCode", by.y = "ZIP")

# Merge in num_of_parks
allPreds <- merge(allPreds, num_of_parks, by.x = "ZIPCode", by.y = "ZIP")

# Merge in liquor
allPreds <- merge(allPreds, liquor, by.x = "ZIPCode", by.y = "Zip Code")

# Merge in grocery
allPreds <- merge(allPreds, grocery, by.x = "Neighborhood", by.y = "Neighborhood")

# write.csv(allPreds, file = "~/420 - Predictive II/Aggregated Data/allPredictors.csv")
```

## Build linear model
```{r}
colnames(allPreds)[colnames(allPreds)=="Liquor Licenses"] <- "Liquor.Licenses"
colnames(allPreds)[colnames(allPreds)=="Grocery Stores"] <- "Grocery.Stores"


names(allPreds)
fit_lm <- lm(Diabetes.related ~ Cancer..All.Sites. + Birth.Rate + Below.Poverty.Level + Parks + Liquor.Licenses + Grocery.Stores, data = allPreds)
summary(fit_lm)

LinearPreds <- cbind(allPreds$Diabetes.related, allPreds$Cancer..All.Sites., allPreds$Birth.Rate, allPreds$Below.Poverty.Level, allPreds$Parks, allPreds$Liquor.Licenses, allPreds$Grocery.Stores)

```

## Fit nnet
```{r}
#standardize all predictors
allPreds1<-allPreds
dropcols <- c("Childhood.Blood.Lead.Level.Screening","Childhood.Lead.Poisoning", "Gonorrhea.in.Females", "Gonorrhea.in.Males")
allPreds1 <- allPreds1[ , !(names(allPreds1) %in% dropcols)]
allPreds1<-sapply(allPreds1[5:33],function(x) (x-mean(x))/sd(x))

```

#Function to make a CV index partition
```{r}
CVInd <- function(n,K) {  #n is sample size; K is number of parts; returns K-length list of indices for each part
  m<-floor(n/K)  #approximate size of each part
  r<-n-m*K  
  I<-sample(n,n)  #random reordering of the indices
  Ind<-list()  #will be list of indices for all K parts
  length(Ind)<-K 
  for (k in 1:K) {
    if (k <= r) kpart <- ((m+1)*(k-1)+1):((m+1)*k)   
    else kpart<-((m+1)*r+m*(k-r-1)+1):((m+1)*r+m*(k-r)) 
    Ind[[k]] <- I[kpart]  #indices for kth part of data
    }
    Ind
}
```

#Check R squared
```{r}
library(nnet)

out<-nnet(Diabetes.related ~ . -Diabetes.related, allPreds1, linout=T, skip=F, size=10,decay=.1,maxit=1000,trace=F)
y <- allPreds1[,12]
yhat<-as.numeric(predict(out))
e<-y-yhat
c(sd(y),sd(e))
1-var(e)/var(y)


```

#Check ALE plots
```{r}
library(ALEPlot)
yhat <- function(X.model, newdata) as.numeric(predict(X.model, newdata))
par(mfrow=c(2,4))

for (j in 1:28)  {ALEPlot(allPreds1[], out, pred.fun=yhat, J=j, K=50, NA.plot = TRUE)
  rug(allPreds1[,j+1]) }  ## This creates main effect ALE plots for all predictors
par(mfrow=c(1,1))
```




## Fit decision tree
```{r}
library(rpart)

control    <-  rpart.control(minbucket  =  5,  cp  =  0.000000001,  xval  =  10,  maxsurrogate  =  0,  usesurrogate = 0) # choose cp as small as you can. This cp corresponds to the largest tree to grow.
out  <- rpart(Diabetes.related ~ . -Diabetes.related, data = allPreds[,5:37], method = 'anova', control = control)

plotcp(out)

bestcp  <-  out$cptable[which.min(out$cptable[,"xerror"]),"CP"]  #  cp  parameter  with  minimal
bestcp

```
```{r}
out1 <- prune(out, cp= .004)
plot(out1); text(out1)
```


```{r}
y <- allPreds$Diabetes.related
yhat<-as.numeric(predict(out1))
e<-y-yhat
c(sd(y),sd(e))
1-var(e)/var(y)
```


```{r}
printcp(out1)


rpart(formula = Diabetes.related ~ ., data = allPreds, method = "anova",     control = control)

out1$variable.importance

plot(yhat,e)
```

